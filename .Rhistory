if(is.null(upper_bound)) {
upper_bound <- c()
for(i in 1:length(start)) {
upper_bound <- c(upper_bound, ifelse(start[[i]] < 0, start[[i]] - 0.5*start[[i]], start[[i]] + 0.5*start[[i]]))
}
}
# Optimization:
res = optimize(data, start, formulas, verbose, lower_bound, upper_bound)
invisible(res)
}
ans=spm_time_dep(data[,2:6], formulas=list(at="a", f1t="f1", Qt="Q*exp(theta*t)", ft="f", bt="b", mu0t="mu0*exp(theta*t)"),
start=list(a=-0.05, f1=80, Q=2e-8, f=80, b=5, mu0=1e-5, theta=0.08))
# returns string w/o leading whitespace
trim.leading <- function (x)  sub("^\\s+", "", x)
# returns string w/o trailing whitespace
trim.trailing <- function (x) sub("\\s+$", "", x)
# returns string w/o leading or trailing whitespace
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
optimize <- function(data, starting_params,  formulas, verbose, lower_bound, upper_bound) {
final_res <- list()
# Current results:
results <- list()
N <- dim(data)[1]
at <- NULL#; a <- starting_params$a;
f1t <- NULL#; f1 <- starting_params$f1;
Qt <- NULL#; Q <- starting_params$Q;
ft <- NULL#; f <- starting_params$f;
bt <- NULL#; b <- starting_params$b;
mu0t <- NULL#; mu0 <- starting_params$mu0;
#theta <- starting_params$theta
variables <- c()
# Assigning parameters:
#---
parameters = trim(unlist(strsplit(formulas$at,"[\\+\\*\\(\\)]",fixed=F)))
parameters = parameters[which(!(parameters %in% c("t","exp")))]
for(p in parameters) {assign(p,NULL, envir = .GlobalEnv); variables <- c(variables, p);}
#----
parameters = trim(unlist(strsplit(formulas$f1t,"[\\+\\*\\(\\)]",fixed=F)))
parameters = parameters[which(!(parameters %in% c("t","exp")))]
for(p in parameters) {assign(p,NULL, envir = .GlobalEnv); variables <- c(variables, p);}
#---
parameters = trim(unlist(strsplit(formulas$Qt,"[\\+\\*\\(\\)]",fixed=F)))
parameters = parameters[which(!(parameters %in% c("t","exp")))]
for(p in parameters) {assign(p,NULL, envir = .GlobalEnv); variables <- c(variables, p);}
#---
parameters = trim(unlist(strsplit(formulas$ft,"[\\+\\*\\(\\)]",fixed=F)))
parameters = parameters[which(!(parameters %in% c("t","exp")))]
for(p in parameters) {assign(p,NULL, envir = .GlobalEnv); variables <- c(variables, p);}
#---
parameters = trim(unlist(strsplit(formulas$bt,"[\\+\\*\\(\\)]",fixed=F)))
parameters = parameters[which(!(parameters %in% c("t","exp")))]
for(p in parameters) {assign(p,NULL, envir = .GlobalEnv); variables <- c(variables, p);}
#---
parameters = trim(unlist(strsplit(formulas$mu0t,"[\\+\\*\\(\\)]",fixed=F)))
parameters = parameters[which(!(parameters %in% c("t","exp")))]
for(p in parameters) {assign(p,NULL, envir = .GlobalEnv); variables <- c(variables, p);}
variables <- unique(variables)
for(p in names(starting_params)) {
results[[p]] <- starting_params[[p]]
assign(p, starting_params[[p]], envir = globalenv())
}
comp_func_params <- function(astring, f1string, qstring, fstring, bstring, mu0string) {
at <<- eval(bquote(function(t) .(parse(text = astring)[[1]])))
f1t <<- eval(bquote(function(t) .(parse(text = f1string)[[1]])))
Qt <<- eval(bquote(function(t) .(parse(text = qstring)[[1]])))
ft <<- eval(bquote(function(t) .(parse(text = fstring)[[1]])))
bt <<- eval(bquote(function(t) .(parse(text = bstring)[[1]])))
mu0t <<- eval(bquote(function(t) .(parse(text = mu0string)[[1]])))
}
iteration <- 0
L.prev <- 0
maxlik_t <- function(data, params) {
stopflag <- FALSE
if(verbose)
cat("Iteration: ", iteration, "\n")
for(p in names(starting_params)) {
assign(p, params[[p]], envir = globalenv())
results[[p]] <<- params[[p]]
if(verbose)
cat(paste(p, get(p)), " ")
}
sigma_sq <- function(t1, t2) {
# t2 = t_{j}, t1 = t_{j-1}
ans <- bt(t1)*(t2-t1)
ans
}
m <- function(y, t1, t2) {
# y = y_{j-1}, t1 = t_{j-1}, t2 = t_{j}
ans <- y + at(t1)*(y - f1t(t1))*(t2 - t1)
ans
}
mu <- function(y, t) {
ans <- mu0t(t) + (y - ft(t))^2*Qt(t)
}
for(i in 1:length(results)) {
if(length(intersect(results[[i]],c(lower_bound[i], upper_bound[i]))) >= 1) {
cat("Parameter", names(results)[i], "achieved lower/upper bound. Process stopped.\n")
cat(results[[i]],"\n")
stopflag <- TRUE
break
}
}
L <- 0
if(stopflag == FALSE) {
for(i in 1:N) {
delta <- data[i,1]
t1 <- data[i, 2]; t2 <- data[i, 3]
ind <- ifelse(is.na(data[i, 5]), 0, 1)
log_s <- -1*(mu(data[i, 4], t2-t1))
if(ind == 0) {
L <- L + (1 -delta)*(-1*log_s) + delta*(1-log_s)
} else {
L <- L + 0.5*pi*(-1*log(sigma_sq(t1, t2)) - (data[i,5] - m(data[i,4], t1, t2))^2/(2*sigma_sq(t1, t2)))
}
}
assign("results", results, envir=.GlobalEnv)
iteration <<- iteration + 1
L.prev <<- L
if(verbose) {
cat("\n")
cat(paste("L", L.prev), "\n")
}
} else {
cat("Optimization stopped. Parametes achieved lower or upper bound.\nPerhaps you need more data or these returned parameters might be enough.\n")
print("###########################################################")
L <- NA
}
L
}
comp_func_params(formulas$at, formulas$f1t, formulas$Qt, formulas$ft, formulas$bt, formulas$mu0t)
optim_results <- NA
# Check if function parameters are consistent to those provided in starting list:
if( setequal(names(starting_params), variables) == FALSE) {
stop("Provided set of function parameters is not equal to that one provided in starting list or vise-versa.")
}
if(verbose == TRUE) {
cat("Variables:\n")
cat(variables,"\n")
cat("Functions:\n")
print(at)
print(f1t)
print(Qt)
print(ft)
print(bt)
print(mu0t)
}
## Optimization:
#result <- optim(par = unlist(starting_params),
#              fn=maxlik_t, dat = as.matrix(data), control = list(fnscale=-1, trace=T, maxit=10000, factr=1e-16, ndeps=c(1e-12, 1e-12, 1e-12,1e-12,1e-16,1e-12,1e-12,1e-12,1e-12)),
#              method="L-BFGS-B", lower = c(-0.5, -0.5, -1, 0,1e-12,1e-6,1e-6,1e-6, 1e-4), upper = c(0, 3, 0, Inf, 1e-7, Inf, Inf, 1, 0.1))
#print(unlist(starting_params))
tryCatch(optim(par = unlist(starting_params),
fn=maxlik_t, dat = as.matrix(data), control = list(fnscale=-1, trace=T, maxit=10000, factr=1e-16),
method="L-BFGS-B",
lower=lower_bound,
upper=upper_bound),
error=function(e) {print(e)},
finally=NA)
final_res <<- list(results, optim_results)
final_res
}
#' spm_time_dep : a function that can handle time-dependant coefficients:
#' @param start : a list of starting parameters, default: llist(a=-0.5, f1=80, Q=2e-8, f=80, b=5, mu0=1e-5, theta=0.08),
#' @param formulas : a list of formulas that define age (time) - dependency. Default: list(at="a1*t+a2", f1t="f1", Qt="Q*exp(theta*t)", ft="f", bt="b", mu0t="mu0*exp(theta*t)")
#' @return optimal coefficients
#' @examples
#' library(spm)
#' Data preparation:
#' N <- 1000
#' data <- simdata_cont(N=N, aH=-0.05, f1H=80, QH=2e-8, fH=80, bH=5, mu0H=2e-5, thetaH=0.08)
#' opt.par <- spm_time_dep(data[,2:6], formulas=list(at="a", f1t="f1", Qt="Q*exp(theta*t)", ft="f", bt="b", mu0t="mu0*exp(theta*t)"), start=list(a=-0.5, f1=80, Q=2e-8, f=80, b=5, mu0=1e-5, theta=0.08))
#' opt.par
spm_time_dep <- function(data,
start=list(a1=-0.5, a2=0.2, f1=80, Q=2e-8, f=80, b=5, mu0=1e-5, theta=0.08),
formulas=list(at="a1*t+a2", f1t="f1", Qt="Q*exp(theta*t)", ft="f", bt="b", mu0t="mu0*exp(theta*t)"),
verbose=TRUE,
lower_bound=NULL, upper_bound=NULL) {
# Values for lower/upper boundaries could be:
#lower_bound=c(-1, 0, 2e-9, 0, 0, 0, 0, 0), upper_bound=c(-0.001, Inf, 1e-5, 1e-5, Inf, Inf, 1e-3, Inf)
# Lower and upper boundaries calculation:
if(is.null(lower_bound)) {
lower_bound <- c()
for(i in 1:length(start)) {
lower_bound <- c(lower_bound, ifelse(start[[i]] < 0, start[[i]] + 0.5*start[[i]], start[[i]] - 0.5*start[[i]]))
}
}
if(is.null(upper_bound)) {
upper_bound <- c()
for(i in 1:length(start)) {
upper_bound <- c(upper_bound, ifelse(start[[i]] < 0, start[[i]] - 0.5*start[[i]], start[[i]] + 0.5*start[[i]]))
}
}
# Optimization:
res = optimize(data, start, formulas, verbose, lower_bound, upper_bound)
invisible(res)
}
ans=spm_time_dep(data[,2:6], formulas=list(at="a", f1t="f1", Qt="Q*exp(theta*t)", ft="f", bt="b", mu0t="mu0*exp(theta*t)"),
start=list(a=-0.05, f1=80, Q=2e-8, f=80, b=5, mu0=1e-5, theta=0.08))
ans=spm_time_dep(data[,2:6], formulas=list(at="a+gt", f1t="f1", Qt="Q*exp(theta*t)", ft="f", bt="b", mu0t="mu0*exp(theta*t)"),
start=list(a=-0.05, gt=9, f1=80, Q=2e-8, f=80, b=5, mu0=1e-5, theta=0.08))
a <- makeCacheMatrix(c(2,3,5,7))
## Put comments here that give an overall description of what your
## functions do
## Write a short comment describing this function
# This function construct a cached matrix object.
# Example:
# a <- makeCacheMatrix()
# a$set(matrix(c(1,2,3,4,5,6,7,8,800),ncol=3))
# a
makeCacheMatrix <- function(x = matrix()) {
inv <- NULL
set <- function(y) {
if(!all(is.na(x))) {
print(y)
print(x)
if(!identical(y,x)) {
cat("Setting the value of the matrix...\n")
x <<- y
inv <<- NULL
} else {
cat("Provided matrix is equal to the previous one. Nothing to set.\n")
}
} else {
x <<- y
inv <<- NULL
}
}
get <- function() x
setinverse <- function(inverse) inv <<- inverse
getinverse <- function() inv
ans <- list(set = set, get = get,
setinverse = setinverse,
getinverse = getinverse)
ans
}
## Write a short comment describing this function
# This functon performs a cached matrix inverse.
# If inverse was previously calculated or
# matrix is the same, it returns
# a cached value of inverse.
# If not, it calculates the matrix inverse and returns it.
cacheSolve <- function(x, ...) {
## Return a matrix that is the inverse of 'x'
inv <- x$getinverse()
if(!is.null(inv)) {
message("getting cached data")
return(inv)
}
data <- x$get()
inv <- solve(data, ...)
x$setinverse(inv)
inv
}
a <- makeCacheMatrix(c(2,3,5,7))
cacheSolve(a)
a <- makeCacheMatrix(c(2,3,5,7,ncol=2))
cacheSolve(a)
a <- makeCacheMatrix(c(2,3,5,7),ncol=2))
a <- makeCacheMatrix(matrix(c(2,3,5,7),ncol=2))
cacheSolve(a)
a <- makeCacheMatrix(matrix(c(2,3,5,7),ncol=2))
a_inversed <- cacheSolve(a)
a_inversed
b <- makeCacheMatrix(matrix(c(2,3,5,7),ncol=2))
cacheSolve(b)
cacheSolve(a)
b <- makeCacheMatrix(matrix(c(2,3,5,7),ncol=2))
b <- makeCacheMatrix(matrix(c(2,3,5,7),ncol=2))
b.set(matrix(c(2,3,5,7))
)
b.set(matrix(c(2,3,5,7)),ncol=2)
b$set(matrix(c(2,3,5,7)),ncol=2)
b$set(matrix(c(2,3,5,7),ncol=2)
)
b$set(matrix(c(2,3,5,7),ncol=2))
a$set(matrix(c(2,3,5,77),ncol=2))
a_inversed <- cacheSolve(a)
a_inversed
a$set(matrix(c(2,3,5,77),ncol=2))
a_inversed <- cacheSolve(a)
a_inversed
## Put comments here that give an overall description of what your
## functions do
## Write a short comment describing this function
# This function construct a cached matrix object.
# Example:
# a <- makeCacheMatrix()
# a$set(matrix(c(1,2,3,4,5,6,7,8,800),ncol=3))
# a
makeCacheMatrix <- function(x = matrix()) {
inv <- NULL
set <- function(y) {
if(!all(is.na(x))) {
if(!identical(y,x)) {
cat("Setting the value of the matrix...\n")
x <<- y
inv <<- NULL
} else {
cat("Provided matrix is equal to the previous one. Nothing to set.\n")
}
} else {
x <<- y
inv <<- NULL
}
}
get <- function() x
setinverse <- function(inverse) inv <<- inverse
getinverse <- function() inv
ans <- list(set = set, get = get,
setinverse = setinverse,
getinverse = getinverse)
ans
}
## Write a short comment describing this function
# This functon performs a cached matrix inverse.
# If inverse was previously calculated or
# matrix is the same, it returns
# a cached value of inverse.
# If not, it calculates the matrix inverse and returns it.
cacheSolve <- function(x, ...) {
## Return a matrix that is the inverse of 'x'
inv <- x$getinverse()
if(!is.null(inv)) {
message("getting cached data")
return(inv)
}
data <- x$get()
inv <- solve(data, ...)
x$setinverse(inv)
inv
}
## Example of how to run:
a <- makeCacheMatrix(matrix(c(2,3,5,7),ncol=2))
a_inversed <- cacheSolve(a)
a_inversed
# Setting a new value:
a$set(matrix(c(2,3,5,77),ncol=2))
a_inversed <- cacheSolve(a)
a_inversed
# Testing if matrix and its inverse are equal to the previous ones:
a$set(matrix(c(2,3,5,77),ncol=2))
a_inversed <- cacheSolve(a)
a_inversed
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
library(lattice)
library(datasets)
data(airquality)
p <- xyplot(Ozone ~ Wind | factor(Month), data = airquality)
print(p)
?points
?text()
?lines
axis
?axis
?lattice
library(datasets)
data(airquality)
qplot(Wind, Ozone, data = airquality, geom = "smooth")
library(ggplot2)
qplot(Wind, Ozone, data = airquality, geom = "smooth")
airquality = transform(airquality, Month = factor(Month))
airquality
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
qplot(Wind, Ozone, data = airquality)
?ggplot2
library(ggplot2)
g <- ggplot(movies, aes(votes, rating))
print(g)
g
qplot(votes, rating, data = movies)
qplot(votes, rating, data = movies) + geom_smooth()
qplot(votes, rating, data = movies, smooth = "loess")
qplot(votes, rating, data = movies) + stats_smooth("loess")
qplot(votes, rating, data = movies) + geom_smooth()
?panel.lmline
2+3
library(tools)
?tools
?file.exists()
?file.exists()
write.table(file = "tidy.data.txt",x = tidy.dataset, row.names = FALSE)
tidy.dataset <- ddply(data.mean.std, c("activity","subjId"), numcolwise(mean))
write.table(file = "tidy.data.txt",x = tidy.dataset, row.names = FALSE)
# Getting and cleaning data class project
library(plyr)
# Setting the work directory:
setwd("~/Dropbox/Data Scientist Specialization/Getting and cleaning data/Project/")
# 0: Downloading the dataset:
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip", "Dataset.zip", method="curl")
unzip("Dataset.zip")
# 1: Merging the training and the test sets in order to create one data set.
## Reading the training files:
x.train <- read.table("UCI HAR Dataset/train/X_train.txt")
y.train <- read.table("UCI HAR Dataset/train/y_train.txt")
subject.train <- read.table("UCI HAR Dataset/train/subject_train.txt")
data.train <- data.frame(x.train, y.train, subject.train)
### Reading the test files:
x.test <- read.table("UCI HAR Dataset/test/X_test.txt")
y.test <- read.table("UCI HAR Dataset/test/y_test.txt")
subject.test <- read.table("UCI HAR Dataset/test/subject_test.txt")
data.test <- data.frame(x.test, y.test, subject.test)
## Merging the train and test data sets:
data <- rbind(data.train, data.test)
# 2: Extracting the measurements on the mean and standard deviation for each measurement.
## Reading the feature names:
features <- read.table("UCI HAR Dataset/features.txt")
##Extracting only the measurements on the mean and standard deviation for each measurement:
mean.id <- grep("mean()",features$V2, ignore.case = F) # Column Ids for means
std.id <- grep("std()",features$V2, ignore.case = F) # Column Ids for standard deviations
ids <- c(mean.id, std.id) # Column Ids for both mean and standard deviation
data.mean.std <- cbind(data$V1.1, data$V1.2, data[, ids]) # Only means and standard deviations for each measurement
names(data.mean.std) <- c("activity", "subjId", as.character(droplevels(features$V2[ids])))
## 3: Use descriptive activity names to name the activities in the data set.
activity.names <- read.table("UCI HAR Dataset/activity_labels.txt")
data.mean.std$activity <- tolower(activity.names$V2[data.mean.std$activity])
## 4: Appropriately label the data set with descriptive variable names.
clean.features <- names(data.mean.std)
clean.features <- gsub("-", "", clean.features)
clean.features <- gsub("\\(\\)", "", clean.features)
names(data.mean.std) <- c(clean.features)
## 5: From the data set in step 4, create a second, independent tidy data set with the average of
# each variable for each activity and each subject.
tidy.dataset <- ddply(data.mean.std, c("activity","subjId"), numcolwise(mean))
## 6: Output the tidy data:
write.table(file = "tidy.data.txt",x = tidy.dataset, row.names = FALSE)
# Getting and cleaning data class project
library(plyr)
# Setting the work directory:
setwd("~/Dropbox/Data Scientist Specialization/Getting and cleaning data/GettingAndCleaningDataProject/")
# 0: Downloading the dataset:
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip", "Dataset.zip", method="curl")
unzip("Dataset.zip")
# 1: Merging the training and the test sets in order to create one data set.
## Reading the training files:
x.train <- read.table("UCI HAR Dataset/train/X_train.txt")
y.train <- read.table("UCI HAR Dataset/train/y_train.txt")
subject.train <- read.table("UCI HAR Dataset/train/subject_train.txt")
data.train <- data.frame(x.train, y.train, subject.train)
### Reading the test files:
x.test <- read.table("UCI HAR Dataset/test/X_test.txt")
y.test <- read.table("UCI HAR Dataset/test/y_test.txt")
subject.test <- read.table("UCI HAR Dataset/test/subject_test.txt")
data.test <- data.frame(x.test, y.test, subject.test)
## Merging the train and test data sets:
data <- rbind(data.train, data.test)
# 2: Extracting the measurements on the mean and standard deviation for each measurement.
## Reading the feature names:
features <- read.table("UCI HAR Dataset/features.txt")
##Extracting only the measurements on the mean and standard deviation for each measurement:
mean.id <- grep("mean()",features$V2, ignore.case = F) # Column Ids for means
std.id <- grep("std()",features$V2, ignore.case = F) # Column Ids for standard deviations
ids <- c(mean.id, std.id) # Column Ids for both mean and standard deviation
data.mean.std <- cbind(data$V1.1, data$V1.2, data[, ids]) # Only means and standard deviations for each measurement
names(data.mean.std) <- c("activity", "subjId", as.character(droplevels(features$V2[ids])))
## 3: Use descriptive activity names to name the activities in the data set.
activity.names <- read.table("UCI HAR Dataset/activity_labels.txt")
data.mean.std$activity <- tolower(activity.names$V2[data.mean.std$activity])
## 4: Appropriately label the data set with descriptive variable names.
clean.features <- names(data.mean.std)
clean.features <- gsub("-", "", clean.features)
clean.features <- gsub("\\(\\)", "", clean.features)
names(data.mean.std) <- c(clean.features)
## 5: From the data set in step 4, create a second, independent tidy data set with the average of
# each variable for each activity and each subject.
tidy.dataset <- ddply(data.mean.std, c("activity","subjId"), numcolwise(mean))
## 6: Output the tidy data:
write.table(file = "tidy.data.txt",x = tidy.dataset, row.names = FALSE)
# Getting and cleaning data class project
library(plyr)
# Setting the work directory:
setwd("~/Dropbox/Data Scientist Specialization/Getting and cleaning data/GettingAndCleaningDataProject/")
# 0: Downloading the dataset:
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip", "Dataset.zip", method="curl")
unzip("Dataset.zip")
# 1: Merging the training and the test sets in order to create one data set.
## Reading the training files:
x.train <- read.table("UCI HAR Dataset/train/X_train.txt")
y.train <- read.table("UCI HAR Dataset/train/y_train.txt")
subject.train <- read.table("UCI HAR Dataset/train/subject_train.txt")
data.train <- data.frame(x.train, y.train, subject.train)
### Reading the test files:
x.test <- read.table("UCI HAR Dataset/test/X_test.txt")
y.test <- read.table("UCI HAR Dataset/test/y_test.txt")
subject.test <- read.table("UCI HAR Dataset/test/subject_test.txt")
data.test <- data.frame(x.test, y.test, subject.test)
## Merging the train and test data sets:
data <- rbind(data.train, data.test)
# 2: Extracting the measurements on the mean and standard deviation for each measurement.
## Reading the feature names:
features <- read.table("UCI HAR Dataset/features.txt")
##Extracting only the measurements on the mean and standard deviation for each measurement:
mean.id <- grep("mean()",features$V2, ignore.case = F) # Column Ids for means
std.id <- grep("std()",features$V2, ignore.case = F) # Column Ids for standard deviations
ids <- c(mean.id, std.id) # Column Ids for both mean and standard deviation
data.mean.std <- cbind(data$V1.1, data$V1.2, data[, ids]) # Only means and standard deviations for each measurement
names(data.mean.std) <- c("activity", "subjId", as.character(droplevels(features$V2[ids])))
## 3: Use descriptive activity names to name the activities in the data set.
activity.names <- read.table("UCI HAR Dataset/activity_labels.txt")
data.mean.std$activity <- tolower(activity.names$V2[data.mean.std$activity])
## 4: Appropriately label the data set with descriptive variable names.
clean.features <- names(data.mean.std)
clean.features <- gsub("-", "", clean.features)
clean.features <- gsub("\\(\\)", "", clean.features)
names(data.mean.std) <- c(clean.features)
## 5: From the data set in step 4, create a second, independent tidy data set with the average of
# each variable for each activity and each subject.
tidy.dataset <- ddply(data.mean.std, c("activity","subjId"), numcolwise(mean))
## 6: Output the tidy data:
write.table(file = "tidy.data.txt",x = tidy.dataset, row.names = FALSE)
